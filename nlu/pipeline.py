import csv
import os
from typing import List, Dict, Tuple, Any, Optional

try:
    from underthesea import word_tokenize
except ImportError:
    def word_tokenize(text: str):
        return text.split()

try:
    from underthesea import ner as uts_ner
except ImportError:
    uts_ner = None

try:
    from .preprocess import normalize_text as ext_normalize_text
    from .preprocess import tokenize_and_map as ext_tokenize_and_map
except ImportError:
    ext_normalize_text = None
    ext_tokenize_and_map = None

try:
    from .intent import IntentDetector
except ImportError:
    IntentDetector = None

try:
    from .entities import EntityExtractor
except ImportError:
    EntityExtractor = None

from config import DATA_DIR, get_intent_threshold

DEFAULT_INTENT_THRESHOLD = get_intent_threshold()


def _normalize_text(text) -> str:
    if ext_normalize_text is not None:
        return ext_normalize_text(text)
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    return text.lower().strip()


def _load_synonyms(path: str) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    if not os.path.isfile(path):
        return mapping

    with open(path, newline="", encoding="utf-8") as f:
        reader = csv.reader(f)
        next(reader, None)
        for row in reader:
            if not row or (row[0] and row[0].strip().startswith('#')) or len(row) < 3:
                continue
            entity, canonical, alias = row[0].strip(), row[1].strip(), row[2].strip()
            if entity == "entity" or not alias or not canonical:
                continue
            alias_norm, canonical_norm = _normalize_text(alias), _normalize_text(canonical)
            if alias_norm and canonical_norm:
                mapping[alias_norm] = canonical_norm
    return mapping


class NLPPipeline:
    def __init__(self, data_dir: str = DATA_DIR, intent_threshold: float = DEFAULT_INTENT_THRESHOLD) -> None:
        self.data_dir = data_dir
        self.intent_threshold = intent_threshold
        self.syn_map = _load_synonyms(os.path.join(data_dir, "synonym.csv"))
        self.intent_samples = self._load_intent_samples(os.path.join(data_dir, "intent.csv"))


        self._intent_detector: Optional[IntentDetector] = (
            IntentDetector(self.intent_samples, self.intent_threshold)
            if IntentDetector is not None else None
        )
        self._entity_extractor: Optional[EntityExtractor] = (
            EntityExtractor(self.data_dir, os.path.join(data_dir, "entity.json"), self.syn_map)
            if EntityExtractor is not None else None
        )

    def _load_intent_samples(self, path: str) -> Dict[str, List[List[str]]]:
        intent_to_samples: Dict[str, List[List[str]]] = {}
        if not os.path.isfile(path):
            return intent_to_samples

        with open(path, newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for r in reader:
                utt = _normalize_text(r.get("utterance") or "")
                intent = (r.get("intent") or "").strip()
                if not utt or not intent:
                    continue
                toks = ext_tokenize_and_map(utt, self.syn_map) if ext_tokenize_and_map else utt.split()
                intent_to_samples.setdefault(intent, []).append(toks)
        return intent_to_samples

    def detect_intent(self, text: str) -> Tuple[str, float]:
        if self._intent_detector is None:
            return "fallback", 0.0
        return self._intent_detector.detect(text, self.syn_map, _normalize_text)

    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        if self._entity_extractor is None:
            return []
        return self._entity_extractor.extract(text)

    def analyze(self, text: str) -> Dict[str, Any]:
        intent, score = self.detect_intent(text)
        entities = self.extract_entities(text)

        return {"intent": intent, "score": score, "entities": entities}
